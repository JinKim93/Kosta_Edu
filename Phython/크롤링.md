## 크롤링(crawling) vs 스크래핑(scraping) vs 파싱(parsing)

- **크롤링이란** 단어는 웹 크롤러(crawler)라는 단어에서 시작한 말이다</br>
크롤러란 조직적, 자동화된 방법으로 월드와이드 웹을 탐색하는 컴퓨터 프로그램이다</br>
크롤링은 크롤러가 하는 작업을 부르는 말로, 여러 인터넷 사이트의 페이지(문서, html 등)를 수집해서 분류하는 것이다</br>
대체로 찾아낸 데이터를 저장한 후 쉽게 찾을 수 있게 인덱싱한다.

- **파싱이란** 어떤 페이지(문서, html 등)에서 내가 원하는 데이터를 특정 패턴이나 순서로 추출하여 정보를 가공하는 것이다</br>
위 문장만 보면 굉장히 간단해 보이지만 컴퓨터 과학적 정의를 보면 파싱이란 일련의 문자열을 의미있는 토큰(token)으로 분해하고,</br>
이들로 이루어진 파스 트리(parse tree)를 만드는 과정을 말한다.
인터프리터나 컴파일러의 구성 요소 가운데 하나로, 입력 토큰에 내제된 자료 구조를 빌드하고 문법을 검사하는 역할을 한다.

- **스크래핑이란** HTTP를 통해 웹 사이트의 내용을 긁어다 원하는 형태로 가공하는 것이다</br>
쉽게 말해 웹 사이트의 데이터를 수집하는 모든 작업을 뜻한다</br>
크롤링도 일종의 스크래핑 기술이라고 할 수 있다.

## 이디야 가격 스크래핑
1. 구글 코랩 사용
2. mount -> 구글 드라이브 -> content디렉토리에 idiya.html 파일 옮기기
 
```py
from google.colab import drive
drive.mount('/content/drive')
```

3. BeautifulSoup 라이브러리 실행
 
```py
from bs4 import BeautifulSoup

page = open("/content/ediya.html","r",encoding= 'utf-8')
html_doc = page.read()
page.close()

soup = BeautifulSoup(html_doc,'html.parser')
result = soup.find_all('p',class_='each-menu') # p태그로 되고, each-menu로 되있는걸 뽑아와라(ediya.html에서)

for data in result:
  print(data.text)
```

### 출력결과
![image](https://user-images.githubusercontent.com/82345970/163772671-671fef87-09c0-4b78-990e-51af0f8ff191.png)

## 당근마켓 텍스트 크롤링 및 특정데이터 파싱
```py
import requests
from bs4 import BeautifulSoup

webpage = requests.get("https://www.daangn.com/hot_articles")
soup = BeautifulSoup(webpage.content, "html.parser")

print(soup)
```
### 출력결과
![image](https://user-images.githubusercontent.com/82345970/163773838-a75558c2-ea37-4115-ac6d-50370bcd411c.png)

## 당근마켓 특정데이터(p태그) 파싱
```py
import requests
from bs4 import BeautifulSoup

webpage = requests.get("https://www.daangn.com/hot_articles")
soup = BeautifulSoup(webpage.content, "html.parser")

print(soup.p.string) #print(soup.p)로 출력하면 <p></p>태그 나온다. 안나오게 할려면 string
```

### 출력결과
![image](https://user-images.githubusercontent.com/82345970/163773963-622c94ff-f7ee-4aa1-8383-921fc0753687.png)

### 당근마켓 특정데이터(h1) 파싱
```py
import requests
from bs4 import BeautifulSoup

webpage = requests.get("https://www.daangn.com/hot_articles")
soup = BeautifulSoup(webpage.content, "html.parser")

print(soup.h1)
```

### 출력결과
![image](https://user-images.githubusercontent.com/82345970/163775474-df4e0ad0-575c-46c9-93c4-497705b24d99.png)

### 당근마켓 처음 찾은 ul태그 하위(children)에 있는걸 파싱
```py
import requests
from bs4 import BeautifulSoup

webpage = requests.get("https://www.daangn.com/hot_articles")
soup = BeautifulSoup(webpage.content, "html.parser")

for child in soup.ul.children: # 처음 찾은 ul태그에서 내부(children)만 출력
    print(child)
```

### 출력결과
![image](https://user-images.githubusercontent.com/82345970/163776073-ec4a28b2-2c43-45f7-b101-ca087f35ace8.png)


### 당근마켓 h2로시작하는 태그 모두다 출력
```py
import requests
from bs4 import BeautifulSoup

webpage = requests.get("https://www.daangn.com/hot_articles")
soup = BeautifulSoup(webpage.content, "html.parser")

print(soup.find_all("h2")) # 당근마켓 h2로시작하는 태그 모두다 출력
```

### 출력결과
![image](https://user-images.githubusercontent.com/82345970/163776922-a1e85a3a-adcf-4381-91ec-6e013f04744b.png)

### 당근마켓 ol, ul 포함된 걸 찾아라 
- re.compile 사용
 
```py
import re
import requests
from bs4 import BeautifulSoup

webpage = requests.get("https://www.daangn.com/hot_articles")
soup = BeautifulSoup(webpage.content, "html.parser")

print(soup.find_all(re.compile("[ou]l"))) #ol, ul 포함된 걸 찾아라 
```

### 출력결과
![image](https://user-images.githubusercontent.com/82345970/163777542-3561f56b-ee0c-440e-a313-7bfde3024541.png)


### 당근마켓 h1 ~ h9 까지 찾아라 
```py
import re
import requests
from bs4 import BeautifulSoup

webpage = requests.get("https://www.daangn.com/hot_articles")
soup = BeautifulSoup(webpage.content, "html.parser")

print(soup.find_all(re.compile("h[1-9]"))) #h1 ~ h9 까지 찾아라 
```

### 출력결과
![image](https://user-images.githubusercontent.com/82345970/163777748-dc7affb0-8f30-49eb-bac1-2bd8c5d71833.png)
